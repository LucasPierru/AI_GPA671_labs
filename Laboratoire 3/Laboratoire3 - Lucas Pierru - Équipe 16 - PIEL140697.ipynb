{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucas Pierru - PIEL14069708 - Équipe 16 - Laboratoire 3 GPA671"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X, y = datasets.make_blobs(n_samples=[100, 100, 1000], cluster_std=[1.0, 2.5, 0.5], random_state=17)\n",
    "# On sépare des points en trois clusters. n_samples correspond au nombre de points dans chaque clusters\n",
    "# cluster_std correspond à l'écart entre chaque point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init(X_train, k):\n",
    "    rands = np.random.choice(len(X_train), k, replace = False) # On choisit aléatoirement 3 points \n",
    "                                                               # parmis tous ceux disponibles\n",
    "    return X_train[rands] # On retourne ces points comme étant nos centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (10):\n",
    "    rand_center = random_init(X,3)\n",
    "    plt.scatter(rand_center[:,0], rand_center[:,1], marker = i) #Bleu, vert, jaune, rouge, violet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les centres sont souvent en haut à droite du graphique et donc proche du 3ème cluster. Ceci est logique vu que nous avons 1000 données pour ce cluster contre 100 pour chacun des autres et donc comme le choix des points est aléatoire, on a plus de chance de tomber sur un point de ce cluster là."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_plusplus(X_train,k):\n",
    "    centers = np.zeros((k,len(X_train[0])))\n",
    "    rand = np.random.choice(len(X_train), 1) # On choisit le premier point aléatoirement\n",
    "    centers[0] = X_train[rand][0] # On le définit comme notre étant notre premier centre\n",
    "\n",
    "    for i in range(1,k):\n",
    "        distance = X_train - np.reshape(centers,(k,1,len(X_train[0])))\n",
    "        distances = np.sqrt(np.sum((distance)**2, axis=2)) # On calcul les distances avec les centres        \n",
    "        min_dist = np.min(distances[:i],axis=0) # On gardes les distances minimales\n",
    "        prob = min_dist/np.sum(min_dist) # On calcul la probabilité en fonction de la distance\n",
    "        rand = np.random.choice(len(X_train), 1, p = prob) # On choisit notre nouveau centre aléatoirement avec celle-ci\n",
    "        centers[i] = X_train[rand]\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (5):\n",
    "    rand_center = kmeans_plusplus(X,3)\n",
    "    plt.scatter(rand_center[:,0], rand_center[:,1], marker = i) #Bleu, vert, jaune, rouge, violet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que les centres sont mieux répartis dans les différents clusters. Cela n'est pas parfait mais cet initialisation semble meilleur que l'initialisation aléatoire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class K_moyennes:\n",
    "    def __init__(self, k:int, init_choice:bool) -> None:\n",
    "        \"\"\"\n",
    "        Classificateur K-moyennes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        k: int\n",
    "            Paramètre du nombre de clusters que l'on veut avoir.\n",
    "        init_choice : bool\n",
    "            Paramètre du choix du type d'initialisation.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.init_choice = init_choice #0 : rand ; 1 : kmeans_plusplus\n",
    "\n",
    "    def fit(self, X_train:np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Entrainement de l'algorithme K-moyennes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.ndarray\n",
    "            Données d'entrainement. Taille : (# points, ).\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        \n",
    "        # On choisit notre initialisation 0 = random ; 1 = kmeans++\n",
    "\n",
    "        if self.init_choice :\n",
    "            self.centers = kmeans_plusplus(self.X_train, self.k)\n",
    "        else:\n",
    "            self.centers = random_init(self.X_train, self.k)\n",
    "        \n",
    "        self.X_train = np.reshape(self.X_train,(len(self.X_train),1,len(X_train[0]))) # On change la forme pour pouvoir \n",
    "                                                                                      # Broadcaster les données\n",
    "        \n",
    "        self.J_array =  []\n",
    "        self.J = 0\n",
    "        previous_J = 1\n",
    "        margin = 0.01\n",
    "\n",
    "        while (self.J>previous_J+margin) | (self.J<previous_J-margin):\n",
    "            self.norm_pc = (self.norm(self.X_train, self.centers))**2 # On calcul la norme au carré des données d'entrainement\n",
    "                                                                      # moins les centres\n",
    "            self.argmin = np.argmin(self.norm_pc, axis = 1) # On prend l'argument du minimum pour chaque donnée\n",
    "            previous_J = self.J\n",
    "            self.J = np.sum(np.min(self.norm_pc, axis=1)) # On calcul J\n",
    "            sum_x = np.zeros((self.k,len(X_train[0])))  \n",
    "            sum_arg = np.zeros(self.k)\n",
    "\n",
    "            # On calcul nos nouveaux centres\n",
    "\n",
    "            for i in range(self.k):\n",
    "                sum_x[i,:] = np.sum(self.X_train[np.where(self.argmin == i)],axis=0)\n",
    "                sum_arg[i] = np.sum((self.argmin == i)*1)\n",
    "            self.J_array.append(self.J)\n",
    "            self.centers = sum_x/np.reshape(sum_arg,(self.k,1))\n",
    "\n",
    "    def fit_restart(self, n_restart:int, X_train:np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Entrainement de K-moyennes avec restart.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.ndarray\n",
    "            Données d'entrainement. Taille : (# exemples,).\n",
    "        n_restart : int\n",
    "            Nombre de fois que l'on recommence l'entrainement.\n",
    "\n",
    "        \"\"\"\n",
    "        J_min = 10**8\n",
    "        for n in range (n_restart):\n",
    "            self.fit(X_train)\n",
    "            if self.J < J_min:\n",
    "                J_min = self.J\n",
    "                self.centers_restart = self.centers\n",
    "                self.J_array_restart = self.J_array\n",
    "        self.centers = self.centers_restart\n",
    "        self.J_array = self.J_array_restart\n",
    "\n",
    "    def predict(self, X_val:np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Prédiction du cluster en fonction de la distance euclidienne d'un point.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_val : np.ndarray\n",
    "            Données de validation. Taille : (# exemples, ).\n",
    "        \"\"\"\n",
    "        self.X_val = X_val\n",
    "        self.X_val = np.reshape(self.X_val,(len(self.X_val),1,len(self.X_val[0])))\n",
    "        self.diff = self.X_val - self.centers\n",
    "        self.distance = np.sqrt(np.sum((self.diff**2),axis=2)) # On calcul la distance euclidienne des points\n",
    "                                                               # par rapport à chaque centres.\n",
    "        return np.argmin(self.distance, axis = 1)\n",
    "\n",
    "    def norm(self, X, U) -> np.ndarray:\n",
    "        return np.sqrt(np.sum((X-U)**2,axis=2))\n",
    "\n",
    "    def compress_image(self, X_val):\n",
    "        pred = self.predict(X_val)\n",
    "        \n",
    "        return self.centers[pred]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée 2 objets avec les deux initialisations.\n",
    "\n",
    "km0 = K_moyennes(3, 0)\n",
    "km0.fit(X)\n",
    "km1 = K_moyennes(3, 1)\n",
    "km1.fit(X)\n",
    "\n",
    "y_pred0 = km0.predict(X)\n",
    "y_pred1 = km1.predict(X)\n",
    "\n",
    "# On affich les données avec les deux initialisations\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6,8)\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred0, s=30, cmap=plt.cm.Paired)\n",
    "plt.scatter(km0.centers[:,0], km0.centers[:,1], color = 'blue')\n",
    "plt.title('Random')\n",
    "plt.subplot(212)\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred1, s=30, cmap=plt.cm.Paired)\n",
    "plt.scatter(km1.centers[:,0], km1.centers[:,1], color = 'blue')\n",
    "plt.title('Kmeans++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche la courbe de J en fonction de l'initialisation.\n",
    "\n",
    "plt.plot(km0.J_array, color = 'blue', label='Random')\n",
    "plt.plot(km1.J_array, color = 'red', label='Kmeans++')\n",
    "plt.yscale('log')\n",
    "plt.title(\"Valeurs de J en fonction de l'initialisation\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'initialisation aléatoire est moins bonne que celle de kmeans_plusplus car on arrive moins souvent à converger avec celle-ci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = K_moyennes(3, 1)\n",
    "km.fit(X)\n",
    "km_restart = K_moyennes(3, 1)\n",
    "km_restart.fit_restart(5,X) \n",
    "y_pred_restart = km_restart.predict(X)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6,8)\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred0, s=30, cmap=plt.cm.Paired)\n",
    "plt.scatter(km.centers[:,0], km.centers[:,1], color = 'blue')\n",
    "plt.title('No restart')\n",
    "plt.subplot(212)\n",
    "plt.scatter(X[:,0], X[:,1], c=y_pred1, s=30, cmap=plt.cm.Paired)\n",
    "plt.scatter(km_restart.centers[:,0], km_restart.centers[:,1], color = 'blue')\n",
    "plt.title('Restart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que fit_restart nous permet d'avoir des bons centres et donc la convergence dès la première exectution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.data import chelsea, astronaut, coffee\n",
    "\n",
    "# On normalise les données\n",
    "\n",
    "chelsea = chelsea().astype(np.float32) / 255\n",
    "astronaut = astronaut().astype(np.float32) / 255\n",
    "coffee = coffee().astype(np.float32) / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée une fonction qui va compresser les images\n",
    "def compress_image(image, k, nb_restart):\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "    y_len = len(image)\n",
    "    x_len = len(image[0])\n",
    "    z_len = len(image[0][0])\n",
    "    # On reshape l'image pour pouvoir faire l'entrainement.\n",
    "    image_reshaped = np.reshape(image, (y_len*x_len, z_len))\n",
    "\n",
    "    km_image = K_moyennes(k, 1)\n",
    "    km_image.fit_restart(nb_restart, image_reshaped)\n",
    "    image_compressed = km_image.compress_image(image_reshaped)\n",
    "    # On reshape dans le format original\n",
    "    image_compressed = np.reshape(image_compressed, (y_len, x_len, z_len))\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image_compressed)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_image(chelsea,5,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_image(coffee, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compress_image(astronaut, 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir que la dernière image est la moins approprié pour cet algorithme. En effetm les nuances de gris en arrière plan de la photo semble perturber les résultats et compromettre l'image obtenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (4,4)\n",
    "n = 5\n",
    "\n",
    "# On récupére les données d'entrainement et les étiquettes.\n",
    "\n",
    "rand_faces_X  = fetch_olivetti_faces(return_X_y=True)[0]\n",
    "rand_faces_y  = fetch_olivetti_faces(return_X_y=True)[1]\n",
    "\n",
    "rand = np.random.choice(len(fetch_olivetti_faces(return_X_y=True)[0]), n,replace = False)\n",
    "\n",
    "rand_faces_images = fetch_olivetti_faces().images[rand]\n",
    "\n",
    "# On affiche n images aléatoire.\n",
    "\n",
    "for i in range(n):\n",
    "    plt.imshow(rand_faces_images[i], cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(rand_faces_X, rand_faces_y, test_size = 0.6, stratify=rand_faces_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "Gaussian_model = GaussianNB()\n",
    "\n",
    "# On crée un classificateur naïf de bayes pour classifier nos données\n",
    "\n",
    "Gaussian_model.fit(X_train, y_train)\n",
    "Gaussian_score = Gaussian_model.score(X_val,y_val)\n",
    "print(\"Score pour le classificateur Bayésien :\",round(Gaussian_score*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_moy = np.mean(X_train, axis=0)\n",
    "X_center = X_train-X_moy\n",
    "C = np.transpose(X_center)@X_center # On calcul la matrice de covariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd, rho = np.linalg.eigh(C) # On calcul nos vecteurs et valeurs propres.\n",
    "\n",
    "# On classe les valeurs propres en ordre croissant et on classe nos vecteurs correspondant.\n",
    "# On calcul également, la somme cumulée des valeurs propres.\n",
    "\n",
    "lambd_norm = np.linalg.norm(lambd)\n",
    "lambd = lambd[::-1]\n",
    "lambd_norm_sum = np.cumsum(lambd/lambd_norm)\n",
    "rho = rho[:,::-1]\n",
    "plt.plot(lambd_norm_sum, color = 'blue', label='Random')\n",
    "plt.title(\"Somme cummulée des valeurs propres normalisées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La somme cumulée des valeurs propres normalisées correspond à la valeur de reconstruction de l'image et nous permet de deteriner le nombre de vecteur nécessaires. Dans notre exemple on peut voir que cette somme cumulée arrive à son maximum aux alentours de 200 vecteurs et donc avec ces 200 vecteurs (parmis 4096) on peut reconstruire l'image correctement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(np.reshape(rho[:,i],(64,64)), cmap=plt.cm.gray)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_vect = 200\n",
    "\n",
    "# On projette nos données et on reconstruit les images.\n",
    "\n",
    "Z = X_center @ rho[:,:nb_vect]\n",
    "X_reconstruit = Z @ np.transpose(rho[:,:nb_vect]) + X_moy\n",
    "\n",
    "m = 8\n",
    "rand_val = np.random.choice(len(X_reconstruit), m,replace = False)\n",
    "X_reconstruit_rand = X_reconstruit[rand_val]\n",
    "\n",
    "for i in (rand_val):\n",
    "    plt.imshow(np.reshape(X_train[i],(64,64)), cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "\n",
    "for j in range(m):\n",
    "    plt.imshow(np.reshape(X_reconstruit_rand[j],(64,64)), cmap=plt.cm.gray)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus on augmente le nombre de vecteurs plus on va se retrouver proche de l'image originale et inversement. Avec un nombre très faible de vecteur, les images reconstruite se ressemble plus ou moins toutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gaussian_model_reconstruit = GaussianNB()\n",
    "\n",
    "# On recrée un classificateur naïf bayesien avec les données reconstruites\n",
    "\n",
    "nb = 10\n",
    "Z = X_center @ rho[:,:nb]\n",
    "X_reconstruit = Z @ np.transpose(rho[:,:nb]) + X_moy\n",
    "\n",
    "Gaussian_model_reconstruit.fit(X_reconstruit, y_train)\n",
    "Gaussian_score = Gaussian_model_reconstruit.score(X_val,y_val)\n",
    "print(\"Score pour le classificateur Bayésien :\",round(Gaussian_score*100,2),\"%\", \"Pour\",nb,\"vecteurs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La précision est très mauvaise pour 10 vecteurs, on est nettement inférieure à la valeur obtenu à la question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "Gaussian_model_reconstruit = GaussianNB()\n",
    "nb = 145\n",
    "Z = X_center @ rho[:,:nb]\n",
    "X_reconstruit = Z @ np.transpose(rho[:,:nb]) + X_moy\n",
    "\n",
    "Gaussian_model_reconstruit.fit(X_reconstruit, y_train)\n",
    "Gaussian_score = Gaussian_model_reconstruit.score(X_val,y_val)\n",
    "end = time.time()\n",
    "print(\"Temps de calcul :\",round(end - start,2),\"secondes\")\n",
    "print(\"Score pour le classificateur Bayésien :\",round(Gaussian_score*100,2),\"%\", \"Pour\",nb,\"vecteurs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En choisissant d'autres nombres de vecteurs, on peut augmenter la précision obtenu. On peut au maximum se retrouver à 70% de précision ce qui est mieux que ce qu'on a obtenu jusque là. Plus on augmente le nombre de vecteurs plus le temps de calcul sera long. Pour 145 vecteurs, on obtient un temps de calcul de 1.4s environ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# On fait la même chose avec un LDA.\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_reconstruit, y_train)\n",
    "lda_score = lda.score(X_val,y_val)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Temps de calcul :\",round(end - start,2),\"secondes\")\n",
    "\n",
    "print(\"Score pour le LDA :\",round(lda_score*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec le LDA, on obtient un taux d'exactitude bien supérieur à ce qu'on avait avant (le LDA est meilleur d'environ 20%)ainsi qu'un temps d'inférence énormément réduit, le temps de calcul obtenu est de 0.09s. Le LDA est donc une meilleure solution pour notre problème."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3d06bfa3b30dce378ac8c3ccbf32ef9a6383160a0b902a2c7644cb660208a199"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
