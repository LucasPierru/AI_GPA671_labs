{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "202e46c5",
   "metadata": {},
   "source": [
    "# Lucas Pierru (PIEL14069708) - Équipe 16 - Laboratoire 2 - GPA671"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be7759",
   "metadata": {},
   "source": [
    "# Exercice 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a06389",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59015d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981502d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=2, n_informative=1, n_redundant=0, n_clusters_per_class=1, random_state=42, shift=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.8, random_state=42) # On sépare nos données de tel sorte\n",
    "# à avoir un test set correspondant à 80% des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche nos données d'entrainement\n",
    "for i in range (len(y_train)):\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3d934",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c057982",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSVM(object):\n",
    "    def __init__(self, lambd: float, k: int, T: int) -> None:\n",
    "        \"\"\"\n",
    "        Classificateur SVM entrainé avec l'algorithme PEGASOS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lambd : float\n",
    "            Paramètre de compromis entre la maximisation de la marge et le respect des contraintes du SVM.\n",
    "        k : int\n",
    "            Nombre d'exemples aléatoirement choisis parmis les données d'entrainement dans chaque itération.\n",
    "        T : int\n",
    "            Nombre d'itérations d'entrainement.\n",
    "\n",
    "        \"\"\"\n",
    "        self.lambd = lambd\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        self.w = np.random.rand(1,2)\n",
    "        \n",
    "        self.w = normalize_w(self.w, self.lambd)\n",
    "        self.b = 0\n",
    "\n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Entrainement du SVM.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.ndarray\n",
    "            Données d'entrainement. Taille : (# exemples, # entrées).\n",
    "        y_train : np.ndarray\n",
    "            Étiquettes des données d'entrainement. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        Ap = np.empty((0,3))\n",
    "        grad_ft_w = np.zeros((1,2))\n",
    "        grad_ft_b = 0\n",
    "        \n",
    "        for t in range (1,self.T+1):\n",
    "            rands = np.random.choice(len(X_train), self.k, replace = False)\n",
    "\n",
    "            At = np.array(X_train[rands])\n",
    "            At = np.append(At, np.reshape(y_train[rands],(self.k,1)), axis = 1)\n",
    "            score = self.decision_function(At[:,:2])\n",
    "            \n",
    "            Ap = At[:,2] * score < 1\n",
    "            Ap = At*np.transpose(Ap)\n",
    "            Ap = Ap[~np.all(Ap == 0, axis=1)]\n",
    "            \n",
    "            grad_ft_w = self.lambd * self.w -(1/self.k)*np.sum(Ap[:,:2]*np.reshape(Ap[:,2],(len(Ap),1)),axis = 0)\n",
    "            grad_ft_b = -(1/self.k)*np.sum(np.reshape(Ap[:,2],(len(Ap),1)),axis = 0)\n",
    "            \n",
    "            lr = 1/(self.lambd*t)\n",
    "            \n",
    "            w_temp = self.w - lr*grad_ft_w\n",
    "            self.b = self.b - lr*grad_ft_b\n",
    "            self.w = min(1,1/(np.sqrt(self.lambd)*np.linalg.norm(w_temp)))*w_temp\n",
    "\n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Évaluation de la fonction de décision du SVM linéaire : w^T x + B.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        scores : np.ndarray\n",
    "            Score(s) pour chaque donnée d'entrée. Taille : (# exemples,).\n",
    "        \n",
    "        \"\"\"\n",
    "        scores = self.w@np.transpose(X) +self.b\n",
    "        return scores\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fonction de classification du SVM.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : np.ndarray\n",
    "            Étiquette(s) prédite(s) pour chaque donnée d'entrée. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        scores = self.decision_function(X)\n",
    "        \n",
    "        y_pred = scores>0\n",
    "        y_pred = 2*y_pred-1\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "# Cette fonction vérifie que w respecte bien les conditions demandés dans l'énoncé \n",
    "def normalize_w(w, lam):\n",
    "\n",
    "    if ((np.linalg.norm(w))>(1/(np.sqrt(lam)))):\n",
    "\n",
    "            w =  w/ (np.sqrt(lam)*np.linalg.norm(w))\n",
    "            \n",
    "            return w\n",
    "\n",
    "    else:\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d97ab",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362fa28",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lamb = [0.0001,0.001,0.01,0.1,1,10,100]\n",
    "k = [1,2,5,10,20,50,100,200]\n",
    "T = 1000\n",
    "max = 0\n",
    "Tef = np.zeros((len(lamb),len(k)))\n",
    "# On itère parmis tout nos lambda et nos k pour trouver les meilleurs hype-paramètres\n",
    "for i in range (len(lamb)):\n",
    "    for j in range (len(k)):\n",
    "        \n",
    "        L = LinearSVM(lamb[i],k[j],T)\n",
    "        L.fit(X_train, (y_train*2)-1) # On s'asssure également d'avoir des y entre -1 et 1\n",
    "        Te = L.predict(X_val)*((y_val*2)-1)\n",
    "        Te = (Te+1)/2 # On calcule notre Te\n",
    "        \n",
    "        Tef[i][j] = np.sum(Te)/len(Te[0])\n",
    "\n",
    "        if Tef[i][j] > max:\n",
    "            max = Tef[i][j]\n",
    "            kmax = k[j]\n",
    "            lambmax = lamb[i]\n",
    "            \n",
    "print(\"Le meilleur k est :\",kmax,\" avec lambda :\", lambmax,\" et Te :\", max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63b3804",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "# On affiche notre graphique du taux d'exactitude en fonction de k et lambda\n",
    "plt.imshow(Tef, interpolation='bilinear', extent = [np.min(k),np.max(k),np.max(lamb),np.min(lamb)], vmin= 0, vmax = 1, aspect='auto')\n",
    "plt.title(\"Taux d'exactitude\")\n",
    "plt.colorbar()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Lambda')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd3d3e",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bbd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On créer un modèle avec le meilleur lambda et k pour voir la frontière de décision.\n",
    "L = LinearSVM(lambmax,kmax,T)\n",
    "L.fit(X_train, (y_train*2)-1)\n",
    "\n",
    "Te = L.predict(X_val)*((y_val*2)-1)\n",
    "Te = (Te+1)/2\n",
    "        \n",
    "Tef = np.sum(Te)/len(Te[0])\n",
    "\n",
    "print(\"Taux d'exactitude : \",Tef)\n",
    "\n",
    "\n",
    "decision_function = L.decision_function(X_val)\n",
    "\n",
    "# On définit nos vecteurs de support comme les points étant sur la marge\n",
    "support_vector_indices = np.where((np.abs(decision_function[0]) <= 1 + 1e-2)&(np.abs(decision_function[0]) >= 1 - 1e-2))[0]\n",
    "support_vectors = X_val[support_vector_indices]\n",
    "\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], c=y_val, s=30, cmap=plt.cm.Paired)\n",
    "        \n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50)\n",
    ")\n",
    "Z = L.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    colors=\"k\",\n",
    "    # On affiche les courbe pour la frontière de décision = -1, 0 et 1\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    ")\n",
    "# On affiche nos vecteurs de supports avec des cercles\n",
    "plt.scatter(\n",
    "        support_vectors[:, 0],\n",
    "        support_vectors[:, 1],\n",
    "        s=100,\n",
    "        linewidth=0.5,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"k\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139b7403",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3f578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=42) # On va cette fois afficher des points qui forment des lunes\n",
    "X, X_val, y, y_val = train_test_split(X, y, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (len(y)):\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=30, cmap=plt.cm.Paired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c320d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On répète la même opération\n",
    "L2 = LinearSVM(lambmax,kmax,T)\n",
    "L2.fit(X, (y*2)-1)\n",
    "\n",
    "decision_function = L2.decision_function(X_val)\n",
    "\n",
    "Te = L2.predict(X_val)*((y_val*2)-1)\n",
    "Te = (Te+1)/2\n",
    "        \n",
    "Tef = np.sum(Te)/len(Te[0])\n",
    "\n",
    "print(\"Taux d'exactitude : \",Tef)\n",
    "\n",
    "support_vector_indices = np.where((np.abs(decision_function[0]) <= 1 + 1e-2) & (np.abs(decision_function[0]) >= 1 - 1e-2))[0]\n",
    "support_vectors = X_val[support_vector_indices]\n",
    "\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], c=y_val, s=30, cmap=plt.cm.Paired)\n",
    "        \n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50)\n",
    ")\n",
    "\n",
    "Z = L2.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    colors=\"k\",\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=1,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "        support_vectors[:, 0],\n",
    "        support_vectors[:, 1],\n",
    "        s=100,\n",
    "        linewidth=0.5,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"k\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fded3147",
   "metadata": {},
   "source": [
    "# Exercice 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606266c2",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfcd76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelSVM(object):\n",
    "    def __init__(self, lambd: float, k: int, T: int) -> None:\n",
    "        \"\"\"\n",
    "        Classificateur SVM avec noyau entrainé avec l'algorithme PEGASOS.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        lambd : float\n",
    "            Paramètre de compromis entre la maximisation de la marge et le respect des contraintes du SVM.\n",
    "        k : int\n",
    "            Nombre d'exemples aléatoirement choisis parmis les données d'entrainement dans chaque itération.\n",
    "        T : int\n",
    "            Nombre d'itérations d'entrainement.\n",
    "\n",
    "        \"\"\"\n",
    "        self.lambd = lambd\n",
    "        self.k = k\n",
    "        self.T = T\n",
    "        \n",
    "    def fit(self, X_train: np.ndarray, y_train: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Entrainement du SVM.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.ndarray\n",
    "            Données d'entrainement. Taille : (# exemples, # entrées).\n",
    "        y_train : np.ndarray\n",
    "            Étiquettes des données d'entrainement. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        self.y_train = y_train\n",
    "        self.X_train = X_train\n",
    "        self.beta = np.zeros((len(X_train),self.T))\n",
    "        self.gamma = 1/(2*np.var(X_train))\n",
    "        \n",
    "        for t in range (1,self.T):\n",
    "            rands = np.random.choice(len(X_train), self.k, replace = False)\n",
    "\n",
    "            At = np.array(rands)\n",
    "    \n",
    "            for n in range (len(X_train)):\n",
    "                if (n in At) :\n",
    "                    if (((y_train[n]/(self.lambd*t))*np.sum(self.beta[:,t-1]*y_train*self.K(X_train, X_train[n])))<1):\n",
    "                        self.beta[n][t] = self.beta[n][t-1] + 1\n",
    "                    else:\n",
    "                        self.beta[n][t] = self.beta[n][t-1]  \n",
    "                else:\n",
    "                    self.beta[n][t] = self.beta[n][t-1]\n",
    "                    \n",
    "        self.alpha = (1/(self.lambd*self.T))*self.beta[:,self.T-1]\n",
    "        self.S = np.where(self.alpha != 0)[0]\n",
    "        self.support_vector = X_train[self.S]\n",
    "        biais = np.zeros(len(self.S))\n",
    "        \n",
    "        for n in range (len(self.S)):\n",
    "\n",
    "            biais[n] = y_train[self.S[n]]-np.sum(self.alpha*y_train*self.K(X_train[self.S[n]],X_train))\n",
    "\n",
    "        self.b = (1/len(self.S))*np.sum(biais)   \n",
    "    \n",
    "    def decision_function(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Évaluation de la fonction de décision du SVM avec noyau.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X_train : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        scores : np.ndarray\n",
    "            Score(s) pour chaque donnée d'entrée. Taille : (# exemples,).\n",
    "        \n",
    "        \"\"\"\n",
    "        len_X = len(X)\n",
    "        scores = np.zeros(len_X)\n",
    "        for i in range(len_X):\n",
    "            scores[i] = np.sum(self.alpha[self.S]*self.y_train[self.S]*self.K(X[i], self.X_train[self.S]))+self.b\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fonction de classification du SVM.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : np.ndarray\n",
    "            Étiquette(s) prédite(s) pour chaque donnée d'entrée. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        scores = self.decision_function(X)\n",
    "        y_pred = (scores>0)*1\n",
    "        return y_pred\n",
    "    # Fonction qui calcul notre K\n",
    "    def K(self, Xn, Xm):\n",
    "        k = np.exp(-self.gamma*(self.norm(Xn,Xm))**2)\n",
    "        return k\n",
    "    # Fonction qui calcul notre norme\n",
    "    def norm(self, Xn, Xm):\n",
    "        return np.sqrt(np.sum((Xn-Xm)**2,axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec3416",
   "metadata": {},
   "source": [
    "Bloc de test avec des valeurs arbitraire de k et lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f21b709",
   "metadata": {},
   "outputs": [],
   "source": [
    "kn = KernelSVM(0.01, 20, 10)\n",
    "y_signed = 2*y -1\n",
    "kn.fit(X, y_signed)\n",
    "y_pred = kn.predict(X_val)\n",
    "\n",
    "te = np.sum((y_val == y_pred)*1)/len(y_val)\n",
    "\n",
    "print(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417909d",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb = [0.0001,0.001,0.01,0.1,1,10,100]\n",
    "k = [1,2,5,10,20,50,100,200]\n",
    "T = 1000\n",
    "max = 0\n",
    "kmax = 0\n",
    "lambmax = 0\n",
    "\n",
    "Tef = np.zeros((len(lamb),len(k)))\n",
    "# On procède au mêmes étapes qu'à l'exercice 1\n",
    "for i in range (len(lamb)):\n",
    "    for j in range (len(k)):\n",
    "        kn = KernelSVM(lamb[i],k[j],T)\n",
    "        kn.fit(X, y_signed)\n",
    "        y_pred = kn.predict(X_val)\n",
    "        te = np.sum((y_val == y_pred)*1)/len(y_val)\n",
    "        \n",
    "        Tef[i][j] = te\n",
    "        \n",
    "        if te > max:\n",
    "            max = te\n",
    "            kmax = k[j]\n",
    "            lambmax = lamb[i]\n",
    "\n",
    "        print(\"Taux d'exactitude : \",Tef[i][j],\"Avec k :\",k[j],\"et lambda :\",lamb[i])\n",
    "print(\"Le meilleur k est :\",kmax,\"avec lambda :\", lambmax,\"et Te :\", max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682affc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On affiche également notre taux d'exactitude en fonction de k et lambda\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.imshow(Tef, interpolation='bilinear', extent = [np.min(k),np.max(k),np.max(lamb),np.min(lamb)], vmin= 0, vmax = 1, aspect='auto')\n",
    "plt.title(\"Taux d'exactitude\")\n",
    "plt.colorbar()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Lambda')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c2675",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comme à l'exercice précédent on va afficer le graphique du meilleur k et lamda\n",
    "KF = KernelSVM(lambmax,kmax,T)\n",
    "KF.fit(X, y_signed)\n",
    "decision_function = KF.decision_function(X_val)\n",
    "y_pred = KF.predict(X_val)\n",
    "\n",
    "te1 = np.sum((y_val == y_pred)*1)/len(y_val)\n",
    "\n",
    "print(\"Taux d'exactitude : \",te1,\"Avec k :\",kmax,\"et lambda :\",lambmax)\n",
    "\n",
    "support_vectors = KF.support_vector # Cette fois-ci on utilisera les valeurs de S comme indices des vecteurs supports\n",
    "\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], c=y_val, s=30, cmap=plt.cm.Paired)\n",
    "\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50)\n",
    ")\n",
    "\n",
    "Z = KF.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    colors=\"k\",\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "        support_vectors[:, 0],\n",
    "        support_vectors[:, 1],\n",
    "        s=50,\n",
    "        linewidth=1,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"k\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7108a",
   "metadata": {},
   "source": [
    "## Question 4 (Vérification avec sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22639749",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "clf = SVC(C = 1/(4*lambmax))\n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_val, y_pred))\n",
    "\n",
    "decision_function = clf.decision_function(X_val)\n",
    "support_vectors = clf.support_vectors_\n",
    "\n",
    "plt.scatter(X_val[:, 0], X_val[:, 1], c=y_val, s=30, cmap=plt.cm.Paired)\n",
    "        \n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(xlim[0], xlim[1], 50), np.linspace(ylim[0], ylim[1], 50)\n",
    ")\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contour(\n",
    "    xx,\n",
    "    yy,\n",
    "    Z,\n",
    "    colors=\"k\",\n",
    "    levels=[-1, 0, 1],\n",
    "    alpha=0.5,\n",
    "    linestyles=[\"--\", \"-\", \"--\"],\n",
    ")\n",
    "plt.scatter(\n",
    "        support_vectors[:, 0],\n",
    "        support_vectors[:, 1],\n",
    "        s=50,\n",
    "        linewidth=1,\n",
    "        facecolors=\"none\",\n",
    "        edgecolors=\"k\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5491b8f",
   "metadata": {},
   "source": [
    "On peut voir, tout d'abord, que les résultats obtenus par notre classe et l'algorithme PEGASOS sont très similaire à ceux obtenus par la librairie sklearn. Notre frontière de décision et nos marges sont très ressemblante avec la vérification dans sklearn. On peut voir par contre, dans notre graphique que les vecteurs de supports se situe un peu partout dans le celui-ci y compris en dehors des marges. Dans le graphique réalisé à partir de sklearn, on peut voir que les vecteurs de supports sont restraint à l'intérieur des marges. \n",
    "\n",
    "$C$ et $\\lambda$ sont tous les deux des paramètre de régularisation qui s'occupent de trouver un bon compromis entre la maximisation des marges et la minimisation des erreur de classification. On choisit $C=\\frac{1}{4\\lambda}$ car nos graphiques se ressemble beaucoup avec cette condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409d83f2",
   "metadata": {},
   "source": [
    "# Exercice 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d4ccb5",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49760285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d44d19",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6432c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sépare de manière balancée nos données\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[0], data[1],test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149e29b",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c914a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNN(object):\n",
    "    def __init__(self, k: int) -> None:\n",
    "        \"\"\"\n",
    "        Classificateur kNN.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        k : int\n",
    "            Nombre de plus proches voisins utilisés pour classifier un nouveau point.\n",
    "\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Chargement des données.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Données d'entrainement. Taille : (# exemples, # entrées).\n",
    "        y : np.ndarray\n",
    "            Étiquettes des données d'entrainement. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fonction de classification du kNN pour un ou plusieurs points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        probabilities : np.ndarray\n",
    "            Vecteur(s) de probabilités prédits pour chaque données d'entrée. Taille : (# exemples, # classes).\n",
    "\n",
    "        \"\"\"\n",
    "        self.d = np.zeros((len(X), len(self.X_train)))\n",
    "        k_nn_values = np.zeros((len(X),3))\n",
    "        \n",
    "        for i in range(len(X)):\n",
    "            for j in range(len(self.X_train)):\n",
    "                self.d[i][j] = np.sqrt(np.sum((X[i]-self.X_train[j])**2))\n",
    "            for k in range(self.k):\n",
    "                \n",
    "                k_nn_values[i][self.y_train[np.argpartition(self.d[i], self.k)[:self.k]][k]] += 1\n",
    "            \n",
    "        probabilities = k_nn_values/self.k\n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fonction de classification du kNN pour un ou plusieurs points\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : np.ndarray\n",
    "            Étiquette(s) prédite(s) pour chaque donnée d'entrée.. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        prob = self.predict_proba(X)\n",
    "        y_pred = np.argmax(prob, axis=1)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea627ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = kNN(3)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "prob = knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f40fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d770397",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45350ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = np.zeros(10)\n",
    "k_range = range(1,11)\n",
    "# On calcul nos taux d'exactitude pour k allant de 1 à 10\n",
    "for k in k_range:\n",
    "    knn = kNN(k)\n",
    "    knn.fit(X_train,y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    y_diff = y_pred - y_test\n",
    "    \n",
    "    y_diff = (y_diff == 0)*1\n",
    "    \n",
    "    te[k-1] = np.sum(y_diff)/len(y_diff)\n",
    "    \n",
    "    print(\"Pour k = \",k,\", la précision est de :\",round(te[k-1]*100,2),\"%\")\n",
    "\n",
    "plt.plot(k_range, te , color = 'red') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75f748",
   "metadata": {},
   "source": [
    "On peut voir avec que plus on augmente le nombre de voisins plus le taux d'exactitude a tendance à augmenter avec le meilleur taux pour k = 9.\n",
    "Notre dataset est assez petit, donc cela fonctionne assez bien mais on sait qu'avec un plus grand nombre de données, l'apprentissage peut devenir lent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc10b8f",
   "metadata": {},
   "source": [
    "### On verifie avec le modèle dans sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f03bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "k_range = range(1,11)\n",
    "\n",
    "scores = {}\n",
    "scores_list = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    scores[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "    scores_list.append(metrics.accuracy_score(y_test, y_pred))\n",
    "    print(\"Pour k = \",k,\", la précision est de :\",round(scores[k]*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced54888",
   "metadata": {},
   "source": [
    "# Exercice 4 (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8546fb",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaivesBayes(object):\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Classificateur Bayesien.\n",
    "\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Chargement des données.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Données d'entrainement. Taille : (# exemples, # entrées).\n",
    "        y : np.ndarray\n",
    "            Étiquettes des données d'entrainement. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        \n",
    "        self.values = np.unique(self.y_train)\n",
    "        for val in self.values:\n",
    "            exec(f'idx_{val} = np.where(self.y_train == val)')\n",
    "            exec(f'self.X_train_{val} = self.X_train[idx_{val}]')\n",
    "            exec(f'self.X_train_{val}_mean = np.mean(self.X_train_{val}, axis=0)')\n",
    "            exec(f'self.X_train_{val}_std = np.std(self.X_train_{val}, axis=0)')\n",
    "            exec(f'self.prob_{val} = len(idx_{val}[0])/len(self.y_train)')\n",
    "            \n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fonction de classification du kNN pour un ou plusieurs points.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        probabilities : np.ndarray\n",
    "            Vecteur(s) de probabilités prédits pour chaque données d'entrée. Taille : (# exemples, # classes).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        probabilities = np.zeros((len(X),len(self.values)))\n",
    "        \n",
    "        for val in self.values:\n",
    "            exec(f'probabilities_{val} = proba_condition(X, self.X_train_{val}_mean, self.X_train_{val}_std)')\n",
    "            exec(f'probabilities[:,val] = self.prob_{val}*np.prod(probabilities_{val}, axis=1)')\n",
    "        \n",
    "        return probabilities\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Fonction de classification du kNN pour un ou plusieurs points\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : np.ndarray\n",
    "            Donnée(s) d'entrée à classifier. Taille : (# exemples, # entrées).\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y_pred : np.ndarray\n",
    "            Étiquette(s) prédite(s) pour chaque donnée d'entrée.. Taille : (# exemples,).\n",
    "\n",
    "        \"\"\"\n",
    "        prob = self.predict_proba(X)\n",
    "        y_pred = np.argmax(prob, axis=1)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "def proba_condition(x,mean,std):\n",
    "    return (1/(std*np.sqrt(2*np.pi)))*np.exp(-(x-mean)**2/(2*std**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b72069",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7b1c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaivesBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "y_pred = nb.predict(X_test)\n",
    "y_diff = y_pred - y_test\n",
    "    \n",
    "y_diff = (y_diff == 0)*1\n",
    "\n",
    "te = np.sum(y_diff)/len(y_diff)\n",
    "\n",
    "print(\"La précision est de :\",round(te*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8de7ff7",
   "metadata": {},
   "source": [
    "### On vérifie avec sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192271f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model_sk = GaussianNB(priors = None)\n",
    "model_sk.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model_sk.predict(X_test)\n",
    "\n",
    "print(\"La précision est de :\",round(metrics.accuracy_score(y_test, y_pred)*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d2e4d1",
   "metadata": {},
   "source": [
    "On obtient un taux qui est supérieur à la plupart des valeurs obtenues avec le KNN mais quand même inférieur au meilleur. On reste tout de fois dans les mêmes ordres de grandeurs. Le classificateur Bayesien naïf est meilleurs si nos données sont en plusieurs dimensions et si elle sont en grandes quantités. Il fera les calculs plus rapidement et donnera une meilleure précision au final. Il est aussi bien plus simple à implémenter que le K-NN."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
